{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/spaces/MultiTransformer/autogen-tutorials/blob/main/agentchat_RetrieveChat.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termination_msg(msg):\n",
    "        content = msg.get(\"content\")\n",
    "        if content.lower() in (\"exit\", \"quit\", \"stop\", \"done\", \"terminate\"):\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gpt-3.5-turbo-0125'\n",
    "config_file=\"CONFIG_LIST.json\"\n",
    "filter_dict = {\"model\": [model]}\n",
    "config_list = autogen.config_list_from_json(\n",
    "                        env_or_file=config_file,\n",
    "                        filter_dict=filter_dict,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"retrieve_content\",\n",
    "            \"description\": \"retrieve content of the linux kernel source code and documantation for code generation and question answering.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"message\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"retrieve content of the linux kernel source code and documantation for code generation and question answering.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"message\"],\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "    \"config_list\": config_list,\n",
    "    \"timeout\": 120,\n",
    "    \"seed\": 50,\n",
    "}\n",
    "\n",
    "manager_llm_config = {\n",
    "    \"config_list\": config_list,\n",
    "    \"timeout\": 120,\n",
    "    \"seed\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_path=\"rag/Linux/\"\n",
    "persist_directory = \"/tmp/chromadb\"\n",
    "prompt_file1 = \"prompts\\KernelGuru.txt\"\n",
    "prompt_file2 = \"prompts\\PatchPro.txt\"\n",
    "task_file = \"prompts/task.txt\"\n",
    "PROMPT1 = open(prompt_file1).read()\n",
    "PROMPT2 = open(prompt_file1).read()\n",
    "task = open(task_file).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new collection for NaturalQuestions dataset\n",
    "# `task` indicates the kind of task we're working on. In this example, it's a `qa` task.\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    is_termination_msg=termination_msg,\n",
    "    name=\"ragproxyagent\",\n",
    "    system_message=\"\"\"Assistant who has extra content retrieval power with the linux kernel source code and documantation for solving difficult problems.\"\"\",\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False,\n",
    "    human_input_mode=\"ALWAYS\",\n",
    "    # max_consecutive_auto_reply=10,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": rag_path,\n",
    "        \"chunk_token_size\": 2048,\n",
    "        \"model\": model,\n",
    "        \"client\": chroma_client,\n",
    "        \"collection_name\": \"rag_collection\",\n",
    "        \"chunk_mode\": \"one_line\",\n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "        # \"customized_prompt\": PROMPT,\n",
    "        # \"human_input_mode\": \"ALWAYS\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_content(message, n_results=3):\n",
    "        ragproxyagent.n_results = n_results  # Set the number of results to be retrieved.\n",
    "        # Check if we need to update the context.\n",
    "        update_context_case1, update_context_case2 = ragproxyagent._check_update_context(message)\n",
    "        if (update_context_case1 or update_context_case2) and ragproxyagent.update_context:\n",
    "            ragproxyagent.problem = message if not hasattr(ragproxyagent, \"problem\") else ragproxyagent.problem\n",
    "            _, ret_msg = ragproxyagent._generate_retrieve_user_reply(message)\n",
    "        else:\n",
    "            ret_msg = ragproxyagent.generate_init_message(message, n_results=n_results)\n",
    "        return ret_msg if ret_msg else message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autogen.agentchat.contrib.agent_builder import AgentBuilder\n",
    "\n",
    "# builder = AgentBuilder(\n",
    "#                     config_file_or_env=config_file,\n",
    "#                     builder_model=model,\n",
    "#                     agent_model=model)\n",
    "# agent_list, agent_configs = builder.build(PROMPT, llm_config, coding=True)\n",
    "# agent_list.append(ragproxyagent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def start_task(execution_task: str, agent_list: list, llm_config: dict):\n",
    "#     group_chat = autogen.GroupChat(agents=agent_list,\n",
    "#                                     messages=[],\n",
    "#                                     max_round=12\n",
    "#                                 )\n",
    "#     manager = autogen.GroupChatManager(\n",
    "#                             groupchat=group_chat,\n",
    "#                             llm_config=llm_config\n",
    "#             )\n",
    "#     agent_list[0].initiate_chat(manager, message=execution_task)\n",
    "\n",
    "# start_task(\n",
    "#     execution_task=task,\n",
    "#     agent_list=agent_list,\n",
    "#     llm_config=llm_config,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = RetrieveAssistantAgent(\n",
    "    name=\"Kernel_Guru\",\n",
    "    system_message=PROMPT1,\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    is_termination_msg=termination_msg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = RetrieveAssistantAgent(\n",
    "    name=\"Kernel_Guru\",\n",
    "    system_message=PROMPT1,\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2 = RetrieveAssistantAgent(\n",
    "    name=\"Patch_Pro\",\n",
    "    system_message=PROMPT2,\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    is_termination_msg=termination_msg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in [agent1, agent2]:\n",
    "    # register functions for all agents.\n",
    "    agent.register_function(\n",
    "        function_map={\n",
    "            \"retrieve_content\": retrieve_content,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1.reset()\n",
    "agent2.reset()\n",
    "group_chat = autogen.GroupChat(agents=[agent1, agent2],\n",
    "                                messages=[],\n",
    "                                max_round=12,\n",
    "                                speaker_selection_method=\"round_robin\",\n",
    "                            )\n",
    "manager = autogen.GroupChatManager(\n",
    "                        groupchat=group_chat,\n",
    "                        llm_config=manager_llm_config\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mchat_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "Please help to resolve issue in the linux kernel.\n",
      "Bug 218514\n",
      "Summary:\tiwlwifi null pointer dereference when debugfs=off\n",
      "Product:\tDrivers\tReporter:\tdrastic_endurance425\n",
      "Component:\tnetwork-wireless-intel\tAssignee:\tDefault virtual assignee for network-wireless-intel (drivers_network-wireless-intel)\n",
      "Status:\tNEW ---\t \t \n",
      "Severity:\tnormal\tCC:\tdrastic_endurance425\n",
      "Priority:\tP3\t \t \n",
      "Hardware:\tAll\t \t \n",
      "OS:\tLinux\t \t \n",
      "Kernel Version:\t\n",
      "Regression:\tNo\n",
      "Attachments:\tjournalctl log\n",
      "Description: drastic_endurance425 2024-02-20 14:47:42 UTC\n",
      "Created attachment 305900 [details]\n",
      "journalctl log\n",
      "Some commit part of the 6.7 kernel series has resulted in the bootup sequence to issue me an oops. One of my kernel parameters is debugfs=off and NetworkManager exits with a kernel NULL pointer dereference. This issue is present whether I use the vanilla kernel 6.7.5 or the zen kernel. I attached my journalctl\n",
      "what files are involved in the above issue?\n",
      "What fix should be applied to solve the issue?\n",
      "you can use the Rag_Assistant for retriving the linux kernel code files, documantation and tree.\n",
      "Consult about posible solution, Eventually, provide the full code fix for the issue including the diff patch content for each relevant file.\n",
      "Avoid repeating on the same message twice, return TERMINATE when done.\n",
      "Review your solution before TERMINATing.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mKernel_Guru\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: retrieve_content *****\u001b[0m\n",
      "Arguments: \n",
      "{\"message\":\"Please provide the relevant files and documentation related to the iwlwifi null pointer dereference issue when debugfs=off (Bug 218514)\"}\n",
      "\u001b[32m*****************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION retrieve_content...\u001b[0m\n",
      "Trying to create collection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniforge3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_ids:  [['doc_73956', 'doc_162492', 'doc_73988']]\n",
      "\u001b[32mAdding doc_id doc_73956 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_162492 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_73988 to context.\u001b[0m\n",
      "\u001b[33mPatch_Pro\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"retrieve_content\" *****\u001b[0m\n",
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "You must give as short an answer as possible.\n",
      "\n",
      "User's question is: Please provide the relevant files and documentation related to the iwlwifi null pointer dereference issue when debugfs=off (Bug 218514)\n",
      "\n",
      "Context is: │   │   ├── trace_functions.c\n",
      "│   │   ├── trace_functions.c\n",
      "│   │   ├── tracing_map.c\n",
      "\n",
      "\n",
      "\u001b[32m*************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mKernel_Guru\u001b[0m (to chat_manager):\n",
      "\n",
      "UPDATE CONTEXT\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPatch_Pro\u001b[0m (to chat_manager):\n",
      "\n",
      "UPDATE CONTEXT\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mKernel_Guru\u001b[0m (to chat_manager):\n",
      "\n",
      "UPDATE CONTEXT\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPatch_Pro\u001b[0m (to chat_manager):\n",
      "\n",
      "UPDATE CONTEXT\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mKernel_Guru\u001b[0m (to chat_manager):\n",
      "\n",
      "UPDATE CONTEXT\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPatch_Pro\u001b[0m (to chat_manager):\n",
      "\n",
      "UPDATE CONTEXT\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mKernel_Guru\u001b[0m (to chat_manager):\n",
      "\n",
      "UPDATE CONTEXT\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPatch_Pro\u001b[0m (to chat_manager):\n",
      "\n",
      "UPDATE CONTEXT\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mKernel_Guru\u001b[0m (to chat_manager):\n",
      "\n",
      "UPDATE CONTEXT\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "manager.initiate_chat(manager,\n",
    "                      message=task,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
